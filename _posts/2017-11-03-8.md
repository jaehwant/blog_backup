---
title: "Lecture 1~4. OpenAI Gym and Dummy Q-learning"
description: hunkim 강의를 수강하며 정리한 페이지 입니다.
categories:
 - machinelearning
tags: 머신러닝
---

<!-- more -->

## 들어가며
[Deep Reinforcement Learning을 수강](http://hunkim.github.io/ml/)하며 정리하는 페이지 입니다.
아래의 Lecture를 듣고 필요한 내용만 정리했습니다.
- Lecture 1 : 수업의 개요
- Lecture 2 : OpenAI GYM 게임해보기
- Lecture 3 : Dummy Q-Learning
- Lecture 4 : Q-Learning exploit  exploration and discounted reward


## Reinforcement Learning (강화학습)이란?
강화학습 하면 처음 떠오르는 것이 Dog traning이다. 좋은 행동을 하면 상을주고, 나쁜 행동은 벌을 줘서
훈련시키는 방법이다. 하지만 강아지에게만 유용한 방법은 아니고, 사람들도 비슷한 방법으로 학습하게 된다.
우리가 살아오며 다년간의 칭찬과 꾸중을 통해 어떤 것이 좋은행동이고, 어떤것이 하지 말아야할 행동인지를 배운다.

이런 어떤 Action에 대한 reward를 이용해 학습하는 것이 강화 학습이다.

## reinforcement Learning
![Frozen Lake World (출처:http://hunkim.github.io/ml/)](/assets/images/DRL_01_00.png)
OpenAI GYM의 첫번째 예제인 Frozen Lake world에 대해서 살펴 보자.
Agent(펭귄)을 S(Start point)에서 G(Ground)로 이동시키는 것이목적이고, 이동할때 H(hole)에 빠지지 않도록 해야 한다.

기본 환경은 2개로 나눠진다. Actor(펭귄) 과 Environment(Frozen lake)
여기서 Actor는 환경속에서 행동을하게 되고, 행동에 따라  observation 을 하게되고, 매 순간마다(행동이 끝날때) reward를 받게 된다.

## OpenAI GYM 개발 환경
- https://gym.openai.com/read-only.html

- 기본 개발 환경
> 1) Tensorflow
> sudo apt-get install python-pip python-dev
> pip install tensorflow (or pip install tensorflow-gpu)
> 2) OpenAI GYM
> sudo apt install cmake
> apt-get install zlib | g-dev
> sudo -H pip install gym
> sudo -H pip install gym[atari]

- 설치후 동작 확인 방법

```python
python
>> import tensorflow
>> import gym
>>
```

예제1. Key input을 사용해 직접 게임해보는 방법([예제소스:github](https://github.com/jaehwant/machine_learning_study/blob/master/01_0_play_frozenlake.py))    
윈도우환경 PyCharm에서는 키가 정상적으로 입력이 안되고 CMD 창에서 실행해야 함.
```python
import gym
from gym.envs.registration import register

class _GetchWindows:
    def __init__(self):
        import msvcrt

    def __call__(self):
        import msvcrt
        return msvcrt.getch()


getch = _GetchWindows()

#Macro
LEFT = 0
DOWN = 1
RIGHT = 2
UP = 3

# key mapping
arrow_keys ={
    b'w' : UP,
    b's' : DOWN,
    b'd' : RIGHT,
    b'a' : LEFT}

#register frozenlake with is_slippery false
register(
    id = 'FrozenLake-v3',
    entry_point = 'gym.envs.toy_text:FrozenLakeEnv',
    kwargs={'map_name' : '4x4', 'is_slippery': False}
)

env = gym.make('FrozenLake-v3')
env.render()

while True:
    key = getch()

    if key not in arrow_keys.keys():
        print("Game aborted!", key)
        print(arrow_keys)
        break

    action = arrow_keys[key] #에이전트의 움직임
    state, reward, done, info = env.step(action) #움직임에 대한 결과값들
    env.render() # 화면 출력
    print("State : ", state, "Action:", action, "Reward:", reward, "Info:",info)

    if done: #도착하면 게임을 끝낸다.
        print("Finished with reward", reward)
        break
```

## Q-Learning
![Q-Function (state-action value function) (출처:http://hunkim.github.io/ml/)](/assets/images/DRL_01_01.png)
Q-Function 은 아래와 같은 입력과 Reward를 가지고 있다.   
1) State - 현재의 내 상태   
2) Action - 내가 취할 Action   
3) Quality(reward) - 내가 Action을 취했을 때 받을 수 있는 점수   

예를들면   
Q(s1,LEFT) : 0   
Q(s1,RIGHT) :  0.5   
Q(s1,UP) : 0   
Q(s1,DOWN) : 0.3   

위의 상태가 있다고 하면, 현재 state(위치)는 s1이고, 여기서 4가지 Action을 취할 수 있는데 각 Action을 취했을 때
얻을 수 있는 Reward가 있다. 여기서 어떻게 이동해야 할까? 랜덤으로 이동한다 or Reward가 max인 값으로 이동한다.   
위 처럼 어떻게 이동할지를 결정하는 것을 Q Policy 라고 한다.   
우선 항상 max값을 찾아서 이동하는 방식을 가정해보자.   

## Dummy Q-learning algorithm
![Learning Q Table:optimal policy (출처:http://hunkim.github.io/ml/)](/assets/images/DRL_01_03.png)
기존처럼 argmax(항상 최대 reward를 받는 쪽으로 이동)하는경우, 아래 처럼 최적의 경로를 찾을 수가 없다.
왜냐하면 안가본길에 대해서 reward가 낮을땐 해당 길로 갈 수 없게 되어있다.
그럼 안가본길을 가게 하려면 떻게 해야할까?

## Exploit vs Exploration Problem
기존값을 이용하는 방식과 모험을 하는 방식이 있어야 한다.
즉 새로운 길을 찾으려면 reward가 낮더라도 그 길을 가 봐야 한다.
그럼 Reward가 낮은 길을 가보려면 어떤 방법이 있을까?

## E-Greedy Policy
``` python
e = 0.1
if rand < e :
  a = random
else
  a = argmax(Q(s,a))
```
위의 방법으로 하면, 10%는 랜덤으로 이동해보고, 나머지 90%는 아는길로 간다.

## decaying E-Greedy
``` python
for i in range(1000)
    e = 0.1 /(i+1)
    if random(1) < e:
      a = random
    else :
      a = argmax(S(s,a))
```
E-Greedy와 비슷하지만, 학습이 거듭될 수록 랜덤으로 길을 가는 확률이 줄어든다. 즉 학습 초반부에는 새로운길로 많이 가보고,
학습이 거듭될 수록 새로운길로 가볼 확률이 줄어든다.

## Add Ramdom noise
![Exploit VS Exploration : add random noise (출처:http://hunkim.github.io/ml/)](/assets/images/DRL_01_04.png)
``` python
action = np.argmax(Q[state, :] + np.random.randn(1,env.action_space.n) / (i + 1))
#노이즈를 추가할때 i+1을 통해 반복될 수록 noise의 값을 줄여준다.
```
현재 알고 있는 reward에 random 한 값을 더해버린다. 그렇게하면 reward의 max값이 변해서 argmax를 선택할 때 새로운 길로
갈 수 있게 된다.
E-Greedy와의 차이점은 E-Greedy의 경우 완전한 랜덤한 길로 가게될 확률이 높지만, noise를 추가하는 방식은 2번째,3번째 reward가 높은
길로 갈 수 있는 확률이 높아 지게 된다. 즉 가능성이 높은 길을 가볼 수 있는 확률이 높아지게 된다.

## discounted reward
![Exploit VS Exploration : add random noise (출처:http://hunkim.github.io/ml/)](/assets/images/DRL_01_05.png)
위의 그림처럼 길1번과 길2번이 있을때 어떤 길이 더 좋은길일까요?
당연히 가깝게 갈 수 있는 2번길이 더 좋을 것 입니다. 그럼 Q는 어떻게 더 좋은길을 알 수 있을까요? 양쪽길 모두 Reward가 동일한 1이기때문에
선택할때 에매해지는 문제가 있습니다.(즉 Q입장에서는 어떤길이 더 좋은 길인지 알 수 없습니다.)

이것을 해결할 수 있는 방법이 Discounted reward 입니다.   
![Discounted reward r=0.9 (출처:http://hunkim.github.io/ml/)](/assets/images/DRL_01_05.png)
여기서 빨간색으로 된 값을 보면, 이전에는 양쪽길 모두 1이었지만, reward를 계산할때 discount를 수행하면 두 길의 reward의 값이 달라지는
것을 확인할 수 있습니다. 이렇게 하면 좀 더 가까운 길로 갈 수 있습니다.

```python
dis = .99
num_episodes = 2000

rList = []
for i in range(num_episodes):
    state = env.reset()
    rAll = 0
    done = False

    #The Q-Table learning algorithm
    while not done:

        action = np.argmax(Q[state, :] + np.random.randn(1,env.action_space.n) / (i + 1))
        #Get new state and reward from environment
        new_state, reward, done, _  = env.step(action)

        #Update Q-Table with new knowledge using learning rate
        Q[state, action] = reward + dis * np.max(Q[new_state,:])

        rAll += reward
        state = new_state

    rList.append(rAll)
```
## Converge
실제로 테스트하는 Q 값은 최종적인 값은 아니다. 즉 Q^은 Q값을 Approxmity하는 값이다.
그럼 Q^은 정말 Q값을 Approxmity할까?

그렇게 되려면 아래의 2가지 조건이 만족되어야 한다.
1) In deterministic world (어떤 방향으로 움직였을 때 항상 같은 값을 갖아야 한다)
2) In finite states (상태가 유한해야 한다)

즉 위와 같은 조건이 아니면 Q-learning을 사용할 수 없다는 뜻이 된다.


## Related Article
- Lecture 1~4. OpenAI Gym and Dummy Q-learning (현재 페이지)
- Lecture 5. Q-learning in non-deterministic World(comming soon)
- Lecture 6. Q-Network (comming soon)
- Lecture 7. DQN (comming soon)

## Reference
[모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)   
[예제 소스 Github : https://github.com/jaehwant/machine_learning_study](https://github.com/jaehwant/machine_learning_study)


<!-- Tip

@목차 작성
## 대목차 (오른쪽에 1.대목차 로 보인다.)
### 소목차 (오른쪽에 1.1소목차 로 보인다.)
* 오른쪽 내어쓰기

@링크
[Text](링크주소)
![Text](그림주소)

@코드 삽입 (블럭)

```
노말 블럭 (highlight 없다 .)
```

```javascript
```python
```ruby

{% highlight ruby linenos %}
def foo
  puts 'foo'
end
{% endhighlight %}


@색상강조

`색강조(회색배경)`

@이모지 넣기
웃는 이모지 : :smile:

:bowtie::smile::laughing::blush::smiley::relaxed::smirk:
:heart_eyes::kissing_heart::kissing_closed_eyes::flushed::relieved::satisfied::grin:

@페이지 제목에 사진을 넣기(홈에서 미리보임)
photos:
- http://ww1.sinaimg.cn/mw690/81b78497jw1emfgwkasznj21hc0u0qb7.jpg
- http://ww3.sinaimg.cn/mw690/81b78497jw1emfgwjrh2pj21hc0u01g3.jpg
- http://ww2.sinaimg.cn/mw690/81b78497jw1emfgwil5xkj21hc0u0tpm.jpg
- http://ww3.sinaimg.cn/mw690/81b78497jw1emfgvcdn25j21hc0u0qpa.jpg

@테이블 넣기

| Table Header 1 | Table Header 2 | Table Header 3 |
| --- | --- | --- |
| Division 1 | Division 2 | Division 3 |
| Division 1 | Division 2 | Division 3 |
| Division 1 | Division 2 | Division 3 |

@테그 넣기
tags:
- Foo
- Bar
- Baz

@카테고리 넣기.
categories:
- Foo
- Bar
- Baz

-->
