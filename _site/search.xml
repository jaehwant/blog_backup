<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[루소 - 에밀(1) 식물은 재배로, 인간은 교육으로 만들어진다.]]></title>
      <url>/2018/10/11/13_emile_chapter1/</url>
      <content type="text"><![CDATA[가상의 인물 에밀은?몸과 마음이 모두 건강하지만 특별히 뛰어난 자질은 없는 에밀이라는 고아를, 한 사람의 교사가 그 아이가 태어나서 자라나 결혼하기까지,‘자연’이라고 하는 위대한 스승의 지시에 따라 어떤 식으로 키워나가는지 5편에 걸쳐 소설 형식으로 서술한 것이 이 책의 주된 내용이다.식물은 재배로, 인간은 교육으로 만들어진다.그러므로 인간의 위대한 능력도 사용방법을 모르면 무용지물이다. 약하게 태어났으므로 힘이, 아무것도 없이 태어났으므로 도움이, 분별력 없이 태어났으므로판단력이 필요한 것이다.  이 모든것은 교육으로 얻어진다.아기에게는 어머니의 보호와 관심이 필요하다.어머니와 아이의 의무는 상호적이다. 한편에서 의무를 소홀히 하게되면 다른 편도 태만하게 된다.아이는 당위 이전에 어머니를 사랑해야 한다. 만일 습관에 의해서도 본능이 강화되지 않으면애정은 싹트기 전에 이미 죽어 버리고 결국 자연의 길에서 벗어나게 된다.이와는 반대로 자연의 길을 벗어나도록 유혹하는 경우가 있는데 양육을 게을리 하거나아이를 지나치게 귀하게 대해 오히려 약골로 만드는 경우이다.이는 아이를 일시적인 위험에서는 벗어나게 할 수 있을지는 모르지만 나약한 유년기를 연장시킴으로써후에 더 큰 고통을 감수하는 결과를 초래한다.자연 그대로의 모습으로 아이를 보존하려면 태어나는 순간부터 방치하지 말고 잘 보살펴야 한다. 최고의 유모는 어머니이며 최고의 교사는 아버지 이다.부모는 자신의 직분에따라 혹은 방법에 있어 일치해야 하고 협력해야 한다.아이에게는 유능한 교사보다도 분별력 있는 아버지의 교육이 낫다.가장 좋은 습관은 어떠한 습관에도 물들지 않는 습관이다.아이들에게 꼭 길러주어야 할 유일한 습관은 어떠한 습관에도 물들이지 않는 습관이다.신체에 자연적인 습관을 지니게 함으로써 언제나 자기의 의지로 관철할 수 있도록 하여 일찍부터 앞으로 다가올자유 경쟁의 시기에 대비해서 힘을사용할 능력을 길러주는 것이 좋다.울어야할 이유도 없는데 우는 아이는 없으며, 요청과 명령을 구분해야 한다.아이는 사물의 저항만 받고 인간의 의지에 의한 저항은 받지 않는다면 결코 반항적이지도 않고 성을 내는일도 없이 건강하게 자라난다.그러나 아이가 하자는 대로 하는 것과 아이를 거역하지 않는 것은 큰 차이가 있다.갓난아기의 최초의 울음은 요청이다. 그런데 여기레 주의를 기울이지 않으면 이 울음은 마침내 명령으로 바뀐다.아이는 처음에는 도움을 요청하지만 나중에는 봉사를 요구하게 된다.그래서 처음에는 자신의 나약함 때문에 의타심이 생기지만 나중에는 권력과 지배의 개념이 싹트게 된다.아이가 아무말도 않고 애써 손을 내밀 때에는 거리감이 없기 때문에 손을 닿을 줄 알고 믿고 있는 것이다.그러나 그가 손을 내밀며 칭얼거릴 때에는 그 물건을 가까이 밀어주거나 갖다달라고 명령하는 것이다.첫번째 경우에는 아이를 물건 가까이 데려다 주는 것이 현명하지만 두번째 경우에는 울음소리를 들은척 하지 말라.아이들은 주위 사람들을 제 마음대로 부릴 수 있는 도구로 알게되면 폭군이 된다.이것은 아이의 천부적은 지배욕에서 비롯되는 것이 아니라 어른들이 가르쳐준 탓이다.왜하면 남의 손을 빌어서 행동하는 것, 말 한마디로 세계를 움직일 수 있다는 것을 깨우치는데는 많은 경험이 필요 없기 때문이다.아이를 선량하게 키우려면모든 악은 약한데서 부터 발생한다.그러므로 아이들을 강하게 만들면 그들은 선량해진다.무엇이든 할 수 있는 사람은 결코 나쁜짓을 하지 않는다.특별한 인간을 만들기 위해서 무엇보다 필요한 것은 자연의 질서에 순응하는 것이다.자연의 길에서 벗어나지 않게 하는 원칙  첫번째 원칙, 아이가 자연에게 받은 모든 힘을 사용하도록 내버려두어야 한다.  두번째 원칙, 육체적으로 필요한 것 중 아이에게 부족한 것은 지성이든 힘이든 간에 보충해주어야 한다.  세번째 원칙, 아이를 도와줄 때는 실제로 필요한 일만 한정해서 도와주어야 하고, 환상이나 이유없는 욕망에 동조해서는 안된다.  네번재원칙, 아이의 언어와 표정을 주의 깊게 연구해야 한다.이상의 원칙은 근본정신은 아이에게 진정한 자유는 가능한 많이 부여하고 지배욕은 줄임으로써 독립적으로 행동하도록 하는 반면의타심을 막아 보자는 데 있다. 이렇게 하면 자신의 능력이 미치지 못하는 것에는 헛된 욕구를 느끼지 못할 것 이다.그는 살고 있다. 그러나 자기가 살고 있다는 것을 알지 못한다.유년기의 초기 발달은 모두가 거의 동시에 일어난다.아이들은 말하는 것도, 걷는 것도 거의 동시에 배운다. 이때까지는 태내에 있을 때와 조금도 다를 바가 없다.그때는 아무런 감정이나 관념도 없이 그저 감각만 있을 뿐이다.심지어 자기가 존재하고 있다는 것조차 의식하지 못한다.]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> 독후감 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[글쓰기가 필요하지 않은 인생은 없다.]]></title>
      <url>/2018/09/05/12_everyone_need_to_writing/</url>
      <content type="text"><![CDATA[Prolog나는 글쓰기를 잘하고싶다는 생각을 막연하게 한다. 그러다 조금씩 글쓰기 관련 책을 읽는다.처음 읽은 책은 “유시민의 글쓰기 특강”이었다.이번에 읽은 책은 “글쓰기가 필요하지 않은 인생은 없다”라는 책인데 작가는 글쓰기를 잘쓰는 것이상으로 글쓰기가 주는 다양한 효과들에 대해서 알려준다.나같은 경우는 글쓰기가 나에게 내재되어 있는 창작욕구를 충족시키기 위한 수단이다.작가는 글쓰기를 힐링과 자신을 채우는 시간으로 쓰는 방법을 제시한다.나에게 글쓰기할때 가장 어려운점은 어떤 것을 쓰느냐 인데이 책은 어떤 글을 쓰는지 방향을 제시해주는 책이었다.책에서 제시하는 몇가지 예를 들면 아래와 같다.  마지막으로 하루 10시간 이상 무언가에 몰입한 적은 언제인가?  인생에서 가장간절하게 무언가를 염원한 시기는 언제인가?  그렇다면 무엇을 얼마나 어떻게 염원했는가?  나를 가장 열광시키는 가치 세가지는?  나라는 사람을 표현하는 한줄의 문구를 작성한다면?  스스로 어디까지 성장할 수 있다고 생각하는가?이외에도 다양한 주제들, 상처를 치료하는 글쓰기, 자서전쓰기, 책쓰기 등등 다양한 내용들이 많다.]]></content>
      <categories>
        
      </categories>
      <tags>
        
          <tag> 독후감 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[무인양품 - 보이지 않는 마케팅]]></title>
      <url>/book/2018/08/07/11/</url>
      <content type="text"><![CDATA[무인양품 - 보이지 않는 마케팅무지라는 브랜드는 이미 많은 사람들에게 익숙한 브랜드이다. 무지를 보면 떠오르는 것이 무엇일까? 아마도 간결하다 심플하다 무채색의 컬러로 무난하다라는 느낌이 아닐까MUJI는 “가장 평범한 형태를 지향한다”무지는 심플하고, 단순하게 만든다. 그리고 다른 브랜드와 다르게 “특징이 없는것” 이 특징이다.브랜드란 소비자에게 자사의 상품이 다른 상품과 어떻게 다른지 명확히 인식하기 위한 도구이다. 그래서 일반적인 브랜드는 타사와 다른 자신만의 특징을 호소한다. 반면 MUJI는 다른 많은 브랜드가 차지하고 있지 않은 그 외 “기타”의 포지셔닝을 지향한다. 이것이야말로 MUJI의 진정한 강점이다. 이로써 세상의 수많은 브랜드를 제외한 “기타여백전부”를포괄하는 브랜드가 되었기 때문이다.“이거면 됬어” 라는 사고방식MUJI의 상품은 심플하고 보편적이다. 세계 곳곳의 문화의 벽을 초월한다. 그러기 위해 최대공약수 개념을 활용한다. 되도록 많은 사람들이 ‘좋다’고 생각하는 상품을 개발하려 한다.“이거면 됬어”라는 사고방식으로 제품을 만든다. 이거면됬어라는 사고가 특정 Target에 명확한 제품보다는 보다 많은 사람들이 좋아할 수 있는범용적인 제품을 만들 수 있게 해준다. 이를 통해 합리적 만족도가 큰 시장을 선점할 수 있도록 한다.“이유를 전달한다”MUJI는 왜 이런 상품을 만들었는지 고객에게 반드시 알리려 한다. MUJI의 광고문구도 ‘이유 있게 싸다’이다. MUJI는 이유를 전달함으로 써 MUJI라는 새로운 컨셉을 사람들에게 이해시키려 한다. MUJI는 언제나 설명을 중요시한다.]]></content>
      <categories>
        
          <category> book </category>
        
      </categories>
      <tags>
        
          <tag> 독후감 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Python - if __name__ == "__main__" 은 왜쓰는거지?]]></title>
      <url>/programming/2018/01/17/10/</url>
      <content type="text"><![CDATA[Summary :Python은 자동으로 실행되는 Main 함수가 없습니다. Python은 메인 함수가 없는 대신에 들여쓰기하지 않은코드를실행하게 됩니다. 다만 함수나 클래스는 정의되지만, 바로 실행되지 않습니다.아래 코드와 같이 if 구문은 최상위 코드(Level 0)코드 입니다.if __name__ == ""__main__""    print ""hello""여기서 name 은 현재의 모듈의 이름을 담고 있는 내장 변수 입니다.그런데 차이점은 아래와 같습니다.python B.py로 실행하면, name 은 main 이라는 값이 들어갑니다.만약 Import 되어 사용된다면 name 은 모듈명이 됩니다.예를들면 A.py, B.py가 있다고 해봅시다.A.pydef func():    print("function A.py")print("top-level A.py")if __name__ == "__main__":    print("A.py 실행됨")else:    print("A.py가 임포트됨")    print(__name__)B.pyimport A as oneprint("top-level in B.py")one.func()if __name__ == "__main__":    print("B.py를 직접 실행")    print(__name__)else:    print("B.py가 임포트되어 사용")실행하면 어떻게 될까요?python B.py를 실행해보면 아래와 같이 나옵니다.top-level A.pyA.py가 임포트되어 사용됨Atop-level in B.pyfunction A.pyB.py가 직접 실행__main__]]></content>
      <categories>
        
          <category> programming </category>
        
      </categories>
      <tags>
        
          <tag> python </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tensorflow lite 알아보기]]></title>
      <url>/machinelearning/2018/01/04/9/</url>
      <content type="text"><![CDATA[Summary :  텐서플로 라이트는 모바일 및 IOT환경에서 빠른 추론이 가능하도록 한 텐서플로입니다.  기존 protobuf 방식에서 flatbuffer 로 변경해서 속도 향상  NNAPI(Neural network API) 를 통해 Hardware 가속 지원(Android 8.1 이상에서)  Quantized(양자화)를 통한 모델파일 감소 및 연산속도 개선  Cross platform 지원 (Android &amp; iOS)Introduction to tensorflow lite텐서플로 라이트는 텐서플로의 모바일과 embedded device를 위한 경량화 버젼이다.TFLite는 on-device에서 ML을 low-latency와 small binary size로 동작하게 해준다.또한 hardware acceleration 을 Android Neural Network API를 통해 사용할 수 있도록 해준다.TFLite는 다양한 테크닉을 사용해 low latency와 kernel optimizing 을 사용한다.pre-fused activations, quantized kernels 들을 사용해 작고 빠른 모델을 사용할 수 있게 한다.Tensroflow lite에서는 기존에 사용하던 protocol-buf 대신에 flatbuffer를 사용한다.  Flatbuffer는 오픈소스이며, protocol-buf 대비 차이점 데이터에 엑세스하기 전에 2차 표현된 구문 분석/압축풀기 단계를 거치지 않아도 되고, 코드의 footprint가protocol-buf보다 훨씩 작다.flatbuffer를 쓰는 이유?  데이터 송/수신 시 파싱/언패킹을 하지 않아도 된다.  메모리 효율성이 높고, 빠른 속도를 보장한다.  유연성이 높다(사용하는 데이터 타입에 대한)  사용하기 편리하다.  크로스 플랫폼, 종속성 없이 사용 가능Flat buffer의 성능비교좀더 상세한 설명 및 Flatbuffer를 만드는 방법은 아래의 링크를 참고하세요 Flatbuffer(블로그)Protocol buffer(google developers)Tensorflow lite architecture텐서플로 라이트 아키텍쳐아키텍쳐 간단한 설명  Trained Tensorflow model학습시킨 모델파일을 준비(pb)한다.  Tensorflow Lite Converterconverter를 통해서 pb 파일을 tensorflow lite 방식으로 변경합니다.변경할때 Quantized 를 수행해서 모델의 크기를 줄입니다.  Tensorflow Lite Model file(.tflite or .lite)Convert가 잘동작되었다면 .lite파일이 생성됩니다.(예제에 있는 .tflite는 .lite파일을 이름을 바꿔놓은 것이다)  Interpreter인터프리터는 임베디드 디바이스 및 모바일에서 오버헤드가 낮은 장치에서도 잘 동작하게 만들어져 있습니다.Tensorflow lite는 적은 의존성으로 단순한 장치에서 사용하기 쉽게 만들어져 있습니다.Flatbuffer를 사용하고 Tensorflow lite는 tensorflow에 있는 함수중에서 일부가 지원되고 지원되지 않는 함수도있습니다. 지원하는함수/지원하지않는함수 보기  Android Neural Network APINNAPI는 Android 8.1 이상의 시스템 이미지에서 사용할 수 있습니다.특징은 아래와 같습니다.          속도 : 신경망프로세싱에 특화된 HW를 사용해 범용 CPU보다 훨씬 빠른 계산을 제공한다.      NNAPI System architecture      NNAPI System architecture이제 적용해볼까요?  기존 모델파일(.pb)를 tflite 형식으로 변경(.lite or .tflite)  toco 모듈을 통해 tflite 형식으로 변경          모델파일의 내용을 dump 해서 얼마나 달라졌는지 확인할 수 있다.        변경된 모델을 tensorflow lite Demo앱을 통해 테스트Trained tensorflow Model우선 미리 학습된 model 파일을 가지고 시작하거나, retrain 수행 후 나온 파일을 가지고 할 수 있습니다.모든 모델이 변환된다고 보장하긴 힘들고 현재는 (Mobilenet, Inception, 등등) 구글에서 제공하고 있는모델들에 대해서는 동작이 됩니다. 모든 모델이 되지 않는 이유는 Tenforflow lite에서 convert되는 함수가모든 함수를 지원하지 않고 몇몇 함수들이 빠져 있기 때문 입니다. 어떤함수가 안되는지는 아래 링크에서확인할 수 있습니다. 지원하는함수/지원하지않는함수 보기Tensorflow Lite Converter  우선 bazel 빌드로 toco 모듈을 빌드해야 합니다.bazel build tensorflow/contrib/lite/toco:toco  빌드가 완료되면 bizel-bin으로 toco로 아래의 옵션으로 실행bazel-bin/tensorflow/contrib/lite/toco/toco -- \--input_file=$(pwd)/mobilenet_v1_1.0_224/frozen_graph.pb \--input_format=TENSORFLOW_GRAPHDEF \--output_format=TFLITE \--output_file=/tmp/mobilenet_v1_1.0_224.lite \--inference_type=FLOAT \--input_type=FLOAT \--input_arrays=input \--output_arrays=MobilenetV1/Predictions/Reshape_1 \--input_shapes=1,224,224,3 \하지만 위의 방법(tensorflow lite 페이지에 있는 가이드)으로 하면 실제 Android App에 업로드했을 때잘 동작되지 않습니다. 그래서 옵션중에 아래의 옵션을 추가해야 하고, 모델 파일도 FLOAT이 아닌 QUANTIZED_UINT8의 옵션을추가해야 합니다.--inference_type=QUANTIZED_UINT8 \--inference_input_type=QUANTIZED_UINT8 \--std_values=128 \--default_ranges_min=0 \--default_ranges_max=6 \  Convert 할때 오류발생 및 어떤 차이점이 있는지 확인하는 방법--dump_graphviz=/덤프를 저장할 경로Tensorflow Lite Demo App  데모 앱은 아래의 위치에서 다운받을 수 있습니다.  코드위치Dot 파일이 생성됨.(Graphiz 용 파일)위의 파일을 바로 graphiz를 통해서 확인해도 되지만, 파일이 큰 경우 잘 열리지 않을 수 있어서graphiz를 pdf파일로 변환해서 확인할 수 있습니다.dot -Tpdf -O 파일명.dotTensorflow Lite Model File이제 (.lite 파일 .tflite) 파일이 생성되었습니다..lite 와 .tflite의 차이점은 무었일까요? Demo 앱에서는 모델의 파일명이 .tflite 이고 convert 할때는.lite를 사용하는데요, 결론을 말씀드리면 차이가 없습니다.두개는 같은형식이고 확장자 명만 다르게 한 것 입니다.하지만 tensorflow demo앱에서는 .tflite로 파일명을 변경해주셔야 합니다!왜냐하면 App을 빌드할때 모델파일은 압축을 하면 안되는데요 그때 압축하지 않는 옵션이 tflite만 추가되어 있습니다. 코드위치(build.gradle)aaptOptions {    noCompress "tflite"}아래처럼 형식을 추가할 수 있습니다.aaptOptions {    noCompress "tflite"    noCompress "lite"}이제 모델파일에 맞게 변수를 수정해주면 됩니다. 코드위치(ImageClassifier.java)/** Name of the model file stored in Assets. */private static final String MODEL_PATH = "mobilenet_quant_v1_224.tflite";/** Name of the label file stored in Assets. */private static final String LABEL_PATH = "labels.txt";/** Number of results to show in the UI. */private static final int RESULTS_TO_SHOW = 3;/** Dimensions of inputs. */private static final int DIM_BATCH_SIZE = 1;private static final int DIM_PIXEL_SIZE = 3;static final int DIM_IMG_SIZE_X = 224;static final int DIM_IMG_SIZE_Y = 224;**참고링크Introduction to TensorFlow Lite Neural Networks API Github : TensorFlow Lite Github : TensorFlow Lite Optimizing Converter command-line reference Github : TensorFlow Lite Optimizing Converter command-line examples Github : TensorFlow Compatibility Guide Issue : tensorflow lite: error when convert frozen model to lite format Image Classify Using TensorFlow Lite Train Image classifier with TensorFlow http://androidkt.com/tenserflow-lite/]]></content>
      <categories>
        
          <category> machinelearning </category>
        
      </categories>
      <tags>
        
          <tag> 머신러닝 </tag>
        
          <tag> 텐서플로 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Lecture 1~4. OpenAI Gym and Dummy Q-learning]]></title>
      <url>/machinelearning/2017/11/03/8/</url>
      <content type="text"><![CDATA[들어가며Deep Reinforcement Learning을 수강하며 정리하는 페이지 입니다.아래의 Lecture를 듣고 필요한 내용만 정리했습니다.  Lecture 1 : 수업의 개요  Lecture 2 : OpenAI GYM 게임해보기  Lecture 3 : Dummy Q-Learning  Lecture 4 : Q-Learning exploit  exploration and discounted rewardReinforcement Learning (강화학습)이란?강화학습 하면 처음 떠오르는 것이 Dog traning이다. 좋은 행동을 하면 상을주고, 나쁜 행동은 벌을 줘서훈련시키는 방법이다. 하지만 강아지에게만 유용한 방법은 아니고, 사람들도 비슷한 방법으로 학습하게 된다.우리가 살아오며 다년간의 칭찬과 꾸중을 통해 어떤 것이 좋은행동이고, 어떤것이 하지 말아야할 행동인지를 배운다.이런 어떤 Action에 대한 reward를 이용해 학습하는 것이 강화 학습이다.reinforcement LearningOpenAI GYM의 첫번째 예제인 Frozen Lake world에 대해서 살펴 보자.Agent(펭귄)을 S(Start point)에서 G(Ground)로 이동시키는 것이목적이고, 이동할때 H(hole)에 빠지지 않도록 해야 한다.기본 환경은 2개로 나눠진다. Actor(펭귄) 과 Environment(Frozen lake)여기서 Actor는 환경속에서 행동을하게 되고, 행동에 따라  observation 을 하게되고, 매 순간마다(행동이 끝날때) reward를 받게 된다.OpenAI GYM 개발 환경      https://gym.openai.com/read-only.html    기본 개발 환경          1) Tensorflowsudo apt-get install python-pip python-devpip install tensorflow (or pip install tensorflow-gpu)2) OpenAI GYMsudo apt install cmakeapt-get install zlib | g-devsudo -H pip install gymsudo -H pip install gym[atari]        설치후 동작 확인 방법python&gt;&gt; import tensorflow&gt;&gt; import gym&gt;&gt;예제1. Key input을 사용해 직접 게임해보는 방법(예제소스:github)  윈도우환경 PyCharm에서는 키가 정상적으로 입력이 안되고 CMD 창에서 실행해야 함.import gymfrom gym.envs.registration import registerclass _GetchWindows:    def __init__(self):        import msvcrt    def __call__(self):        import msvcrt        return msvcrt.getch()getch = _GetchWindows()#MacroLEFT = 0DOWN = 1RIGHT = 2UP = 3# key mappingarrow_keys ={    b'w' : UP,    b's' : DOWN,    b'd' : RIGHT,    b'a' : LEFT}#register frozenlake with is_slippery falseregister(    id = 'FrozenLake-v3',    entry_point = 'gym.envs.toy_text:FrozenLakeEnv',    kwargs={'map_name' : '4x4', 'is_slippery': False})env = gym.make('FrozenLake-v3')env.render()while True:    key = getch()    if key not in arrow_keys.keys():        print("Game aborted!", key)        print(arrow_keys)        break    action = arrow_keys[key] #에이전트의 움직임    state, reward, done, info = env.step(action) #움직임에 대한 결과값들    env.render() # 화면 출력    print("State : ", state, "Action:", action, "Reward:", reward, "Info:",info)    if done: #도착하면 게임을 끝낸다.        print("Finished with reward", reward)        breakQ-LearningQ-Function 은 아래와 같은 입력과 Reward를 가지고 있다. 1) State - 현재의 내 상태 2) Action - 내가 취할 Action 3) Quality(reward) - 내가 Action을 취했을 때 받을 수 있는 점수예를들면 Q(s1,LEFT) : 0 Q(s1,RIGHT) :  0.5 Q(s1,UP) : 0 Q(s1,DOWN) : 0.3위의 상태가 있다고 하면, 현재 state(위치)는 s1이고, 여기서 4가지 Action을 취할 수 있는데 각 Action을 취했을 때얻을 수 있는 Reward가 있다. 여기서 어떻게 이동해야 할까? 랜덤으로 이동한다 or Reward가 max인 값으로 이동한다. 위 처럼 어떻게 이동할지를 결정하는 것을 Q Policy 라고 한다. 우선 항상 max값을 찾아서 이동하는 방식을 가정해보자.Dummy Q-learning algorithm기존처럼 argmax(항상 최대 reward를 받는 쪽으로 이동)하는경우, 아래 처럼 최적의 경로를 찾을 수가 없다.왜냐하면 안가본길에 대해서 reward가 낮을땐 해당 길로 갈 수 없게 되어있다.그럼 안가본길을 가게 하려면 떻게 해야할까?Exploit vs Exploration Problem기존값을 이용하는 방식과 모험을 하는 방식이 있어야 한다.즉 새로운 길을 찾으려면 reward가 낮더라도 그 길을 가 봐야 한다.그럼 Reward가 낮은 길을 가보려면 어떤 방법이 있을까?E-Greedy Policye = 0.1if rand &lt; e :  a = randomelse  a = argmax(Q(s,a))위의 방법으로 하면, 10%는 랜덤으로 이동해보고, 나머지 90%는 아는길로 간다.decaying E-Greedyfor i in range(1000)    e = 0.1 /(i+1)    if random(1) &lt; e:      a = random    else :      a = argmax(S(s,a))E-Greedy와 비슷하지만, 학습이 거듭될 수록 랜덤으로 길을 가는 확률이 줄어든다. 즉 학습 초반부에는 새로운길로 많이 가보고,학습이 거듭될 수록 새로운길로 가볼 확률이 줄어든다.Add Ramdom noiseaction = np.argmax(Q[state, :] + np.random.randn(1,env.action_space.n) / (i + 1))#노이즈를 추가할때 i+1을 통해 반복될 수록 noise의 값을 줄여준다.현재 알고 있는 reward에 random 한 값을 더해버린다. 그렇게하면 reward의 max값이 변해서 argmax를 선택할 때 새로운 길로갈 수 있게 된다.E-Greedy와의 차이점은 E-Greedy의 경우 완전한 랜덤한 길로 가게될 확률이 높지만, noise를 추가하는 방식은 2번째,3번째 reward가 높은길로 갈 수 있는 확률이 높아 지게 된다. 즉 가능성이 높은 길을 가볼 수 있는 확률이 높아지게 된다.discounted reward위의 그림처럼 길1번과 길2번이 있을때 어떤 길이 더 좋은길일까요?당연히 가깝게 갈 수 있는 2번길이 더 좋을 것 입니다. 그럼 Q는 어떻게 더 좋은길을 알 수 있을까요? 양쪽길 모두 Reward가 동일한 1이기때문에선택할때 에매해지는 문제가 있습니다.(즉 Q입장에서는 어떤길이 더 좋은 길인지 알 수 없습니다.)이것을 해결할 수 있는 방법이 Discounted reward 입니다. 여기서 빨간색으로 된 값을 보면, 이전에는 양쪽길 모두 1이었지만, reward를 계산할때 discount를 수행하면 두 길의 reward의 값이 달라지는것을 확인할 수 있습니다. 이렇게 하면 좀 더 가까운 길로 갈 수 있습니다.dis = .99num_episodes = 2000rList = []for i in range(num_episodes):    state = env.reset()    rAll = 0    done = False    #The Q-Table learning algorithm    while not done:        action = np.argmax(Q[state, :] + np.random.randn(1,env.action_space.n) / (i + 1))        #Get new state and reward from environment        new_state, reward, done, _  = env.step(action)        #Update Q-Table with new knowledge using learning rate        Q[state, action] = reward + dis * np.max(Q[new_state,:])        rAll += reward        state = new_state    rList.append(rAll)Converge실제로 테스트하는 Q 값은 최종적인 값은 아니다. 즉 Q^은 Q값을 Approxmity하는 값이다.그럼 Q^은 정말 Q값을 Approxmity할까?그렇게 되려면 아래의 2가지 조건이 만족되어야 한다.1) In deterministic world (어떤 방향으로 움직였을 때 항상 같은 값을 갖아야 한다)2) In finite states (상태가 유한해야 한다)즉 위와 같은 조건이 아니면 Q-learning을 사용할 수 없다는 뜻이 된다.Related Article  Lecture 1~4. OpenAI Gym and Dummy Q-learning (현재 페이지)  Lecture 5. Q-learning in non-deterministic World(comming soon)  Lecture 6. Q-Network (comming soon)  Lecture 7. DQN (comming soon)Reference모두를 위한 머신러닝/딥러닝 강의 예제 소스 Github : https://github.com/jaehwant/machine_learning_study]]></content>
      <categories>
        
          <category> machinelearning </category>
        
      </categories>
      <tags>
        
          <tag> 머신러닝 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Neural Network1, XOR 문제와 학습방법 (L08)]]></title>
      <url>/machinelearning/2017/08/05/7/</url>
      <content type="text"><![CDATA[들어가며모두를 위한 머신러닝/딥러닝 강의1 를 수강하며 정리하는 페이지 입니다.  Lecture 8, XOR 문제 딥러닝으로 풀기 입니다.강의정리  hunkim.github.io 강좌 Lecture 8번(XOR 문제 딥러닝으로 풀기) 입니다.Neural Network을 사용해서 XOR 풀기기존에 Machine Learning이 정체되었던 시기에는 XOR 문제에 대해서 Machine Learning을 사용하여 풀 수 없다고 생각하던 시절이 있었습니다. 기존에 문제를 풀어나가던 방식으로는 풀 수 없다는 것이 수학적으로 증명되었습니다. 아마도 그 당시 Computing power 또한 낮았기 때문에 더 풀 수 없었던 것 습니다.      XOR 이란?Exclusive OR 연산으로 두개의 값이 같으면 0 다르면 1이 되는 논리연산 입니다. 위의 그래프를 보면 알 수 있듯이 단순한 직선으로는 XOR를 구분할 수 없습니다.        NN을 이용해서 XOR 풀기 (forward propagation)기존에 사용하던 Logistic regression을 neural net으로 만들어서 푸는방법 물론 이 방법도 예전에는 풀 수 없을 것이라는 확신에 가득한 이야기를 한 사람도 있습니다.          “No one on earth had found a viable way to train - Marvin Minsky”      우선 각 Logistic Regression의 Weight 값과 Bias 값은 미리 정해놓고 NN으로 XOR을 풀 수 있는지 확인해보고 있습니다.위의 그림을 보고 값을 대입해서 풀어보면 NN으로 XOR을 푸는게 가능해 보입니다. 하지만 예제는 답이나올만한 Weight 값과 Bias를 입력했으니 쉽게 나오는 것 같지만, 예전에는 이 문제를 풀기가 어려웠습니다.  간략한 설명 x1 = 0, x2 = 0 일때, (XOR 하면 답은 0이 나옴)  우선 첫번째 neurun에서 x1,x2를 사용하여 y1을 구합니다. 행렬곱을 해야 하는데요, 좀 풀어서 쓰면 아래처럼 됩니다.  x1w + x2w -8, x1과 x2는 0이기 때문에 -8이 나옵니다. -8이라는 값을 sigmoid 함수를 수행하면 0보다 작기 때문에 0이 됩니다. 그래서 y1은 0이 됩니다.  이제 y2를 구해보겠습니다. 마찬가지로 x1w + x2w + 3이됩니다. x1,x2는 0이기 때문에 3이 나오고, 이 값을 sigmoid를 수행하면 0보다 크기 때문에 y2는 1이 됩니다.  자 이제 y1 = 0, y2 = 1이 나왔습니다. 이 값을 마지막 NN에 대입해서 풀어 봅니다. y1w + y2w + 6, 대입해보면 y1은 0이기 때문에 생략하면 -11 +6 이 됩니다. 그럼 -5가 됩니다. 이 값을 Sigmoid를 수행하면 값이 0보다 작기 때문에 0이 나옵니다.  즉 [0,0] 일때 출력은 0이 나왔습니다. (XOR과 동일하죠?)왼쪽의 Network을 matrix를 사용하여 오른쪽의 모양으로 수정할 수 있습니다. 즉 구현할때 두개의 Logistic regression 모양을 따로 생성하는게 아니라 matrix를 사용해서 한개로 묶어서 표현할 수 있습니다.아까는 별개로 있던 network를 matrix를 이용해 하나로 합친 모습입니다.실제로 구현하는 소스에서도 훨씬 간단하게 표현되는 것을 알 수 있습니다.K = tf.sigmoid(tf.matmul(X,W1) +b1)hypothesis = tf.sigmoid(tf.matmul(K,W2) +b2)여기까지는 이미 주어진 값으로 테스트해보는 방법이었습니다.그럼 어떻게 W값과 b값을 자동으로 알아낼 수 있을까요?W값과 b값을 안다는 것은 학습을 통해서 Cost를 계산하고 cost 가 minimize 된 값을 찾는 것을 뜻합니다.이 부분이 어려워서 이 XOR 문제를 풀지 못했는데요. 풀 수 있게 해준 방법이 나왔습니다.바로Backpropagation 입니다. Backpropagation에 대해서는 다음시간에 조금더 자세히 포스팅하겠습니다.Reference            모두를 위한 머신러닝/딥러닝 강의 &#8617;      ]]></content>
      <categories>
        
          <category> machinelearning </category>
        
      </categories>
      <tags>
        
          <tag> 머신러닝 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Machine learning system design (W6-2)]]></title>
      <url>/machinelearning/2017/08/01/6/</url>
      <content type="text"><![CDATA[들어가며Coursera의 Machine Learning1 by Andrew Ng교수님의 강의를 수강하며 정리하는 페이지 입니다. 저도 배우고 있어서 일부 내용에 오류가 있을 수 있습니다. 관련 부분에 대해서 추가되는 내용은 업데이트를 계속 해나갈 생각 입니다.강의정리Week6 의 2번째 챕터 입니다.(Machine Learning System Design) 머신러닝 시스템 설계하는 방법 과 고려해야할 부분에 대해서 설명해주는 강의 입니다.Prioritizing what to work on: Spam classification example스팸필터를 어떻게 만들어야 할까요? 우선 아래의 스팸필터예제를 살펴보겠습니다.스팸을 구분하는데 있어서 다양한 방법이 있겠지만, 단순히 아래처럼 일부단어에 의도적인 오타가 입려력되어 있고, 특정한 단어를 많이 사용한다고 가정 합니다.우선 Supervised learning 방식으로 spam classifier를 만듭니다. 학습데이터인 x는 spam에 사용된 단어로 우선 100개를 선정해서 feature를 만듭니다. y는 스팸인지 아닌지에 대한 결과값을 넣는데 사용 합니다.자 이제 아주 간단한 스팸 필터가 생성되었습니다. 이제 이 스팸필터의 정확성을 높이기 위해서 어떤방법을 사용할 수 있을까요?  많은 양의 데이터를 수집한다. (허니팟2 : 스팸메일을 모아올 가상의 시스템)  스팸메일을 헤더를 이용한다. 그리고 유사단어와 다른단어를 구분할 알고리즘을 개발한다.  잘못된 철자(w4tch)를 처리하는 알고리즘 을 개발한다.하지만 위의 방법중 어떤 것이 가장 좋을지는 선택하기엔 어렵습니다.Error analysis제안 하는 접근 방법  신속하게 구현해본다. 간단한 알고리즘으로 시작하고 교차검증3 데이터를 사용해 테스트해본다  학습곡선을 그려본다. 학습곡선을 통해 데이터가 더 필요한지, feature 가 필요한지 결정할 수 있다.  교차검증데이터를 활용해서 어떤 부분에 오류가 발생했는지 분석해본다.약 500개의 교차검증 셋이 있고, 이 중에서 약 100개가 잘못 분류되었다고 한다면 직접 100개가 잘못 분류된 원인을 분석해볼 수 있습니다.예를들면 잘못분류된 스팸메일의 종류를 분류해볼 수 있습니다.(약물판매,유사제품,피싱메일,그외)또는 의도되 오타(med1cine)를 검출하거나, 이상한 문장부호를 사용하는 것을 찾을 수 있습니다. 위에서도 이야기했지만, 신속하고 간단한 알고리즘으로 시작하는것은 이런 교차검증 셋으로 미쳐 발견하지 못했던 문제점들을 분석하고 찾을 수 있기 때문입니다.The importance of numerical evaluation알고리즘을 평가할때 숫자로 평가할 수 있어야 합니다. 아래의 예를 살펴 보겠습니다. 유사단어에 대해서 stemming software를 사용할 수 있습니다. 예를들면 discount, discounts, discounted, discounting 을 discount 한단어로 처리할 수 있습니다.이런 소프트웨어를 “stemming software45” 라고 합니다. 이렇게 유사단어들에 대해서 stemming software 를 적용한다면, 더 적은 수의 feature를 사용할 수 있습니다.하지만 stemming software 에도 문제점은 있습니다. 예를들면 univers 와 university를 구분하는것은 잘못 구분될 수 있습니다.그럼 이러한 stemming software를 사용해야 할까요? 거기에 대한 판단을 바로 적용했을 때의 결과 에러율을 보고 판단할 수 있습니다.  Stemmer를 사용하지 않았을 때 : 5% 에러  Stemmer를 사용했을 때 : 3% 에러위처럼 적용했을 때의 통계가 사용했을 때 에러율이 더 낮아진다면 사용하는게 좋을 수 있습니다.그리고 강좌에서 Andrew Ng은 머신러닝 시스템을 디자인 할 때, 빠르게 알고리즘을 설계하고 구현하는것이 좋다고 합니다. 초기에 많은 시간을 들이는 것 보다빠르게 구현해보고 모델에 대해 평가해본 후, 부족한 점에 대해서 보완하는 것이 빠르고 시간이 절약된다고 합니다.Error metirics for skewed classes예전에 예로 다루었던 cancer classification을 다시 보겠습니다.logistic regression 모델을 사용해서 1%의 오류율을 달성했다고 합시다. 과연 이 시스템은 잘 만들어진걸까요? 맞을 확률이 99%나 되니까 잘 된걸까요?여기서 입력데이터의 skewed classes의 문제가 발생합니다.  skewed classes : Train 데이터가 한쪽으로 치우쳐져 있는 경우 (예를들면 전체 데이터에서 대부분 암이 아닐때,)내가만든 logistic regression 모델의 에러율은 1%이지만, 실제로 전체 환자의 99.5%가 암이 아닙니다.  즉 모든 training set에 대해서 y= 0(cancel 이 아님)을 나타내도 에러율이 0.5%가 된다는 것입니다.이렇게 skewed 된 class에서는 위와 같은 문제가 발생하게 됩니다. 단순히 에러율만을 가지고는 모델을 평가하기 어렵습니다.그럼 어떻게 해야할까요?에러율 이외에 시스템을 평가할 수 있는방법은  Precision(정밀도) 과 Recall(재현율) matrix 가 있습니다.  링크 : precision 과 recall의 이해  Precision(정밀도) :  검출된 결과가 얼마나 정확한지 여부  Recall(재현율) : 실제 검출되어야할 결과가 얼마나 잘 나왔는지 여부                   Actual class 1(True)      Actual class 0 (False)                  predicted class 1(True)      True positive      False positive              Predicted class 0(False)      False negative      True negative      실제 암환자가 100명중에 5명(5%) 있다고 가정해 봅시다. 제가 만든 linear regression 모델로는 10%의 확률로 검출합니다. 즉 100명의 환자에서 10명이 암이 있다고 예측 했습니다.자 이제 이 결과가 맞다고 할 수 있을까요? 분류하는 것도 중요하지만, 얼마나 정확하게 실제의 데이터를 분류해냈는지도 검토해봐야 합니다.위의 예를 가지고 다시 Metrics 를 그려 보겠습니다.                   Actual class 1(True)      Actual class 0 (False)                  predicted class 1(True)      3      7              Predicted class 0(False)      2      88      실제 암환자의 수는 5명 입니다. 우리의 시스템은 암을 10명 있다고 분류하였습니다.(실제로 10명중에서 3명이 암 입니다.) 그리고 암이 아니라고 분류된 것은 90명 입니다. (실제로 90명 중에서 2명은 암 입니다.)여기서 Precision(정확도) = (True Positive)/(True positive + False Positive) 입니다.  Precision(정확도) = (3) / (3+ 7) = 30%즉 모델에서 예측한 10명중에 실제 암은 3명으로 검출의 정확도가 30%밖에 안되는 것이죠.여기서 Recall(재현율) = (True Positive) / (True Positive + (False Negative)  Recall(재현율) = (3) / (3 + 2) = 60%즉 모델의 결과가 실제 암을 얼마나 찾아 냈는지 여부를 나타 냅니다.Trading off precision and recallPrecision 과 Recall을 모두 만족시킬 수는 없을까요?안타깝게도 precision 과 recall은 Trade off 관계 입니다. 암을 진단하는 Tumer의 사이즈의 임계값을 증가시킨다고 가정해 봅시다. 그렇게되면 암이라고 판단되는 경우는 줄어 들게 됩니다.(암을 구별하는 임계값이 높아졌기 때문에) 이렇게되면 Precision은 증가합니다. 하지만 암을 암으로 판단하지 못하는 경우가 발생하기 때문에Recall은 낮아지게 됩니다. 정확도는 증가하지만, 재현율은 감소되는 것이죠.반대로 암을 판단하는 임계값을 낮춘다고 가정해 봅시다. 임계값이 낮아졌기 때문에 암으로 판단되는 수는 많아 집니다. 당연히 정확도는 감소하게 됩니다.반대로 실제 암을 암이라고 판단할 확률이 높아지기 때문에 잘못 구분할 확률이 줄어들어서 재현율은 높아집니다. 정확도는 낮아졌지만, 재현율은 높아지는 것이죠.그 둘의 상관관계가 위 그림의 오른쪽에 있는 그래프 입니다. Threshold가 0.09이면 실제 암보다 암을 적게 분류하기 때문에(반대로 Error가 감소) 정확도는 높아집니다. 하지만 실제 암을 정확히 분류하지못했기 때문에 재현율은 감소 되죠. Threshold가 0.01이면 실제 암보다 암을 많이 분류하기 때문에(Error가 증가) 정확도가 낮아집니다. 하지만 실제 암은 더 많이 포함되기 때문에재현율은 증가하게 됩니다.그럼 최적의 Threshold는 어떻게 결정해야 할까요?Precision 과 recall 을 조합해서 사용하면 됩니다. 바로 F score 입니다.  F score = 2 * (Precision * Recall) / (Precision + Recall)위의 3개의 알고리즘 중에서 F score 가 높은 Algorithm 1 이 가장 좋다는 것을 알 수 있습니다.Data for machine learning단어나 문장을 완성하는 머신러닝 알고리즘을 만들 때 위와 같은 알고리즘들을 사용할 수 있습니다.(perceptron, Winnow, Memory-based, naive Bayes 등등)그림7의 오른쪽의 그래프를 보면 Training set의 크기에 따라서 점차 정확도가 비슷해진다는 것을 확인할 수 있습니다.즉 좋은 알고리즘을 사용하는 것도 중요하지만, Training set를 많이 보유하는 것도 중요하다는 것을 알 수 있습니다.많은 데이터를 가지고 있는 것도 중요하지만, 그 중에서 좋은 정보가 무었인지 구분하는 것도 중요합니다. 집을 구하는 것을 예를 들면 집의 평수만을 가지고 집값을 대략적으로 예측할 수 는 있습니다. 하지만 크기만이 집값을 결정하는요소는 아닌데요, 다양한 요소들이 있을 수 있습니다. 방의 갯수, 위치, 주변시설등이 있습니다. 이 Feature에 대한 영향력을 어떻게 검증할 수 있을까요?그리고 이 Feature가 영향을 줄 수 있는지 어떻게 알 수 있을까요? 좀더 쉬운 방법중 하나는 사람(전문가)가 주어진 x와 y로 예측할 수 있는지 검토해보는 것 입니다.많은 데이터를 사용해서 알고리즘을 구현할 때 많은 Feature를 사용하는 logistic regression/linear regression 과 많은 수의 hidden unit을 사용하는neural network들은 low bias algorithm 일 가능성이 높습니다.또한 많은 training set 을 사용하게 되면 overfit 되어 Jtest 와 Jtrain이 비슷해질 수 있습니다.이전 강의에서 다룬 bias와 variance에 대해 배운것처럼 알고리즘이 어떤 성향을 보이는지 여부에 따라 알고리즘의 복잡도를 조절하거나, 데이터의 양을 늘림으로서좋은 결과를 나타내는 머신러닝 시스템을 만들 수 있습니다.  링크 : Bias-Variance Tradeoff:경험에서 배울 때 주의사항Reference            Coursera : Machine Learning Stanford University &#8617;              허니팟(honeypot) 해커들을 유인하는 꿀단지, 즉 정보가 있는 것처럼 보이는 가상 시스템 : &#8617;              교차검증(corss-validation data) : 예를들면 Supervised learning 을 위한 학습데이터가 있다면 (60%는 traing set, 20% 는 cross validation set, 20%는 test set으로 사용) &#8617;              stemming software &#8617;              Poter stemmer &#8617;      ]]></content>
      <categories>
        
          <category> machinelearning </category>
        
      </categories>
      <tags>
        
          <tag> 머신러닝 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[머신러닝의 개념 및 용어 (L01)]]></title>
      <url>/machinelearning/2017/06/13/5/</url>
      <content type="text"><![CDATA[들어가며모두를 위한 머신러닝/딥러닝 강의1 를 수강하며 정리하는 페이지 입니다. 저도 공부하는 입장이라서 내용에 오류가 있을 수 있습니다.관련 부분에 대해서 추가되는 내용은 업데이트를 계속 해나갈 생각 입니다.Windows 기반에서 Tensorflow 설치방법전체적인 설치가이드는 Tensorflow 사이트를 참고하면 됩니다.  1) Python 3.5x 버젼 설치 2) Anaconda 설치 3) PyCharm Community Edition 설치머신러닝의 개념 및 용어기존 프로그램은 명확한 규칙과 로직을 가지고 프로그래밍 되었기 때문에, 복잡한 룰을 가지고,규칙을 세우기 어려운 프로그램을 작성하는데는 어려움이 있었습니다. 개발자가 일일이 정하지 않고,학습해서 뭔가를 배우는 능력을 갖는 프로그램을 만드는 것이 머신러닝 입니다.머신러닝은 크게 2가지로 나눌 수 있습니다.Supervised learning트레이닝 셋을 미리 제공하여 학습하는 방법 (고양이 이미지 100장, 개 이미지 100장)을Labeling 하여 data set을 만들고 그 Data를 통해 학습시킬 수 있습니다.대부분의 지도 학습은 비용함수(Cost function)를 통해 결과 값의 오류를 줄여나가는 방식으로 동작한다.지도학습에서는 입력값과 출력값 사이에 관계가 있다고 생각하고 문제를 해결하기 위한 방법으로 주로 사용되는 부분은 회귀분석(Regression) 과 분류(Classification)에 주로 사용된다.  부동산크기와 가격과의 상관관계를 풀때 (Regression)종양을 가진 환자와 종양이 없는 환자를 분류할 때 ( Binary classification) 혹은 학점을 A,B,C,D와 같이 나눠서 분류할 수 있을때(Multi-label classification)을 사용할 수 있다.1)Type of supervised learning  Regression  Binary Classification  Multi-label classificationLinear regression 예제 수행결과# Lab 2 Linear Regressionimport tensorflow as tftf.set_random_seed(777)  #난수를 생성하는데 같은 seed값을 쓰면 해당 난수가 항상 일정한 숫자가 나온다.# 1) 학습시킬 X와 Y값을 주어준다. (x가 1일때, y가 1)x_train = [1, 2, 3]y_train = [1, 2, 3]# tf.Variable == Tensorflow가 사용하는 변수, trainable 한 변수이다.# tensorflow 가 학습시키는 과정에서 사용되는 변수W = tf.Variable(tf.random_normal([1]), name='weight') # random_normal[1] 값이 1개인 1차원 Array를 나타냄b = tf.Variable(tf.random_normal([1]), name='bias')# hypothesis 노드 생성hypothesis = x_train * W + b# cost/loss function# reduce_mean 은 주어진 tensor 에서의 평균값을 내준다. ( t = [1,2,3,4] 일때, reduce_mean을 수행하면 2.5가 나온다.)cost = tf.reduce_mean(tf.square(hypothesis - y_train))# Minimizeoptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)train = optimizer.minimize(cost)# Launch the graph in a session.sess = tf.Session()# 변수를 사용하기 전에는 항상 초기화를 해줘야 한다.sess.run(tf.global_variables_initializer())#tensorboard 에서 Graph 를 확인하기 위한 코드merged = tf.summary.merge_all()writer = tf.summary.FileWriter("./logs/lap-02-1",sess.graph)# Fit the linefor step in range(2001):    sess.run(train)    if step % 20 == 0:        print(step, sess.run(cost), sess.run(W), sess.run(b))예제 수행결과0 2.82329 [ 2.12867713] [-0.85235667]20 0.190351 [ 1.53392804] [-1.05059612]40 0.151357 [ 1.45725465] [-1.02391243]...960 1.46397e-05 [ 1.004444] [-0.01010205]1980 1.32962e-05 [ 1.00423515] [-0.00962736]2000 1.20761e-05 [ 1.00403607] [-0.00917497]위의 모델을 Tensorboard 를 사용하여 Graph를 출력해보면 아래와 같이 나옵니다.Unsupervised learning비지도 학습은 데이터의 상호 유사성을 이용해 공통된 특징을 찾아내는 과정을 통해 찾아내는 방식이다.Reference[1] http://hunkim.github.io/ml/ [2] https://github.com/hunkim/DeepLearningZeroToAll [2] http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html [3] 텐서플로 입문 (예제로 배우는 텐서플로) [4] 텐서플로 첫걸음            모두를 위한 머신러닝/딥러닝 강의 &#8617;      ]]></content>
      <categories>
        
          <category> machinelearning </category>
        
      </categories>
      <tags>
        
          <tag> 머신러닝 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[비즈니스 협상 - 통합적 협상이란]]></title>
      <url>/business/2016/10/28/4/</url>
      <content type="text"><![CDATA[Overview:대학원에서 비즈니스 협상을 수강하며 정리한 노트 입니다. 수업내용을 정리한 것이라서 일부 부족하거나 잘못된 부분이 있을 수 있습니다. 혹시 잘못된 부분이나 궁금한 것이 있으면 아래 Disqus 댓글을 통해 알려주세요 :)통합적 협상의 특성바로 이전에 투쟁적 협상에 대해서 알아봤었는데요  통합적 협상은 서로의 입장에서 생각하는 것에서 벗어나, 근본적인 이해관계를 생각하는 협상입니다.  통합적 협상에서는 모든 협상당사자의 욕구를 충족시키고자 노력하게 되고, 상호간의 공통점 강조 및 차이를 최소화 하려고 합니다.  서로 정보와 아이디어 교환도 활발하고 상호 이익이 되는 대안을 찾기위해서 노력합니다.통합적 협상의 단계  문제를 확인하고 정의하기          양자가 수긍할 수 있는 방식으로 문제를 정의하기      문제해결을 목표로 삼고, 목표의 장애요인을 찾는다.      문제를 객관화 하고 문제정의 과정과 해결과정을 분리한다.        서로 추구하는 이익의 명확화          이익(Interest)과 입장(Position)으로 구분하기        협상의 대안 찾기          타협하기, 통나무타기, 파이바꾸기, 순응의 댓가 줄이기, 순응 등 방법을 사용하기        대안평가 하고 최적 대안 찾기          객관적 지표, 질적 평가, 합의가능성 평가하기      개인의 선호를 명확하기 제시하기 : 위험회피정도, 시간, 가치관등      필요할 경우 냉각기간을 갖기, 최종 합의가 될 때까지 결정을 조건부로 하기      Interest 와 Position을 구분하기Interest 와 Postion은 무었일까요?  Interest : 명시적 요구 속에 숨겨져있는 근원적 욕구  Position : 명시적으로 요구하는 사항위의 2개를 구분하는 쉬운 예가 있는데요,예를 들면 슈퍼마켓에 어떤 손님이 들어와서 “사이다 한병 주세요” 라고 말했다고 가정해 볼게요  여기서 “사이다 한병 주세요” 가 Postion 입니다.손님은 명시적으로 “사이다 한병 주세요” 라고 얘기하고 있어요그럼 여기서 손님의 Interest(숨겨진 욕구)는 무었일까요??  “목이 말라서 시원한 음료를 마시고 싶다” 일꺼에요바로 Interest를 발견하기 위해서는 Why? 를 생각해보면 되는데요,만약에 슈퍼마켓에 사이다가 없을때 손님이 “사이다 주세요” 라고 요구한다면,없다고 그냥 보내시겠어요?아니면 손님의 “목이 말라서 시원한 음료를 먹고싶다”는 내면적 요구를 조금 변경해서 다른 음료를 권할 수 있을까요?  “손님 사이다는 다 떨어졌는데요, 갈증해소하는데 좋은 이온음료가 있습니다. 어떠신가요?”그럼 손님은 사이다를 고집(Position을 고집)하는 경우를 제외한다면 내면적 욕구(필요성)을 만족시킬 수 있는대안이 있다면 해당 대안을 선택할 것입니다.이렇게 협상을 Position 중심에서 Interest 중심으로 이동시켜나가게되면, 결렬될 수 있는 협상이 이루어질 수 있어요 그리고 이 두사람 모두 각자 상호 이익이 되는 대안을 찾게 됩니다. 손님은 갈증을 해소했고, 상인은 물건을 팔게 되는거에요통합적 협상을 성공적으로 만드는 요소들  공통의 목표를 설정하였는가?  상대방과 내가 문제해결 능력이 있는가?  서로의 관점이 다르다는 것을 충분히 숙지하고 있는가?  신뢰와 정확하고 명확한 의사소통Position 과 Interest의 차이!제가 설명하는 것보다 아래의 유튜브 동영상을 한번 보시면 더 이해가 쉬울것같습니다. Youtube 설명자료실습해보기아래의 사례를 통해 통합적 협상으로 발전할 수 있는지? Position을 Interest로 변화시킬 수 있는지 예를 들어 볼게요  A : 오직 캐나다에서만 나오는 특산품 “캐나다오렌지”를 구매하려고 합니다. 캐나다오렌지 껍질에서 추출한 물질을 토양에 뿌리면 아프리카에서도 곡식 수확량을 늘릴 수 있습니다. 아프리카에서는 매년 10만명이 식량 부족으로 죽고 있고, 이번 캐나다오렌지의 입찰을 저에게 매우 중요한 사안 입니다.  B : 오직 캐나다에서만 나오는 특산품 “캐나다오렌지”를 구매하려고 합니다. 캐나다오렌지 속살에서 추출한 물질은 피부주름개선에 사용할 수 있는매우 가치있는 재료 이고, 다른곳에서는 구할 수가 없습니다. 캐나다오렌지를 구매하여 약 5만명이 사용할 수 있는 피부주름개선제를 시중에 판매를시작하려고 합니다. 저희 비즈니스에서는 해당 재료가 매우 중요해서 이번입찰은 꼭 제가 받아야 합니다.위의 A와 B는 서로 입찰한다는 것을 알고 있습니다. 그래서 입찰 전달 한 카페에서 만나기로 하였습니다. 아마도 협상을 통해 가격을 조정하거나 상대를 포기하게 할  수 있을지 모릅니다.위의 상황일 때 과연 누가 낙찰받아야 할까요? 이 협상은 어떻게 진행될까요?Reference강의 : 비즈니스 협상책 : 최고의 협상 (p150 ~ p233)]]></content>
      <categories>
        
          <category> business </category>
        
      </categories>
      <tags>
        
          <tag> 협상 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[비즈니스 협상 - 투쟁적 협상이란]]></title>
      <url>/business/2016/09/28/3/</url>
      <content type="text"><![CDATA[Overview:대학원에서 비즈니스 협상을 수강하며 정리한 노트 입니다. 수업내용을 정리한 것이라서 일부 부족하거나 잘못된 부분이 있을 수 있습니다. 혹시 잘못된 부분이나 궁금한 것이 있으면 아래 Disqus 댓글을 통해 알려주세요 :)투쟁적 협상이란?자신의 몫을 극대화하는 것이 협상의 목적이 되는 경우 (1회성 협상) 이전에 다루었던 협상은 지속적인 상호 이익이 될 수 있는 협상이었다면, 오늘 배운 내용은 1회성의 투쟁적 협상입니다. 1회성의 거래에서 내가 받을 수 있는 최대의 몫을 이끌어 내는 것이 이 협상의 목표 입니다! 일상생활을 예를 든다면, 주택거래와 같(아파트말고, 단독주택일경우)은 상호간에 원하는 가격이 다를 경우 협상을 통해 최대한 얻어내는 것이 있을 수 있습니다.아래의 그림과 같이 투쟁적 협상에서는 구매자와 판매자가 다른 가격대를 원하게 되고, 이들의 협상은 ZOPA 라는 영역에서 진행되게 됩니다.  ZOPA : Zone of Possible Agreement투쟁적 협상을 해야하는 상황은 어떨때 일까요?  자원이 Zero-sum으로 배분되어야 하는 경우  상대의 몫에 대해서 관심이 적은 경우  입장(Position) 중심의 협상인 경우예를들면 주택거래와 같은게 있는데요, 구매자가 원하는 가격, 판매자가 원하는 가격이 다른경우가 많죠그리고 거래에서 상대방에 몫에 대해선 관심이 적고, 각자의 Position을 유지하려고 하는 협상이 있습니다.입장중심의 협상구조  최초제안과 (initial offer) 과 역제안(Asking Price) : 협상당사자들이 제시하는 최초의 가격  목표점(target point) : 협상을 통해 성취하려는 목표 가격, 최적의 목표  저항점(walkaway point) : 최대 양보점으로 이부분을 넘어서면 협상이 결렬될 수 있음  대안(alternative) : BATNA, 즉 제 3자와 별도로 거래를 성사시킴으로써 얻어질 수 있는 대안적 성과투쟁적 협상의 과정 (미리 정한 후 협상에 임해야 함)  최초제안과 역제안을 얼마나 공격적으로 설정할 것인가?  언제 협상을 중단할 것인가?  더 이상 양보할 수 없는 경우는 언제인가?  협상에서 원하는 목표는 무엇인가?  협상에서 원하는 목표는 무엇인가?  최종제안(최후 제시안)은 어느 정도로 할 것인가?투쟁적 협상의 전략  상대방의 저항점 알아내기투쟁적 협상에서의 가장 중요한 점은 바로 상대방의 저항점을 알아내고, 나의 저항점은 숨기는 것입니다.내가 상대의 저항점 가격을 안다면, 해당가격으로 구매할 수 있겠고, 상대가 나의 저항점을 알게된다면 상대 역시해당 가격에 판매할 수 있게 됩니다. 그래서 최대한 자신의 저항점은 숨기고, 상대방의 저항점은 드러나게 해야 합니다.  상대방의 저항가격까지 밀어부치기계속적인 가격인하 요구 및 가격인하가 필요한 이유를 만들어서 계속 압박한다.  상대방의 저항가격을 움직이기상대의 저항가격을 알아냈다면, 이제 저항가격 아래로 움직일 수 있도록 시도해야 합니다.저항가격을 낮출 수 있는 요소는 다양하게 있는데요, 협상결과물에 대해 상대가 특정 성과에 부여하는 가치를 조절하는 일,협상지연이나 어려움에 따라 상대가 지불해야 하는 기대비용을 통해 낮추는 방법진척된 협상을 폐기하는 데 따르는 상대의 기대비용을 통해 낮출 수 있습니다.  예) 만약 내가 협상을 신속하게 해야하고, 이를 연기할 수 없다는 것을 상대방이 알게 된다면, 당연히 상대방은 이점을 통해 좀 더유리한 결과를 얻으려고 할 것입니다. 반대로 내가 협상지연 및 폐기로 인한 비용이 낮다고 상대방에게 확인 시키면, 상대의저항 가격은 그만큼 완화될 것입니다. 이렇게 된다면 나는 서두를 필요가 없고, 기다릴 수 있는 여유가 생깁니다.(최고의 협상 165p참고)  협상 결렬 시의 가치와 협상 이후의 가치를 비교하기  상대방이 협상 목표를 달성하는데 상당한 비용이 소요하도록 만들기  나의 대안이 매력적이게 보이기  외부인을 끌어들여 상대를 위협하기  협상일정을 지연시키기  양보의 크기보다 양보의 패턴이 중요하다최후통첩식의 제안 전술 : 구속력(commitment)상대의 기대나 결정에 영향을 주는 방법으로 구속력 이외에도 경고,위협,허풍,약속과 같은 방법이 있습니다.구속(Commitment)의 목적은 협상가가 의도하는 행동에 대해 모호한 측면을 제거하기 위함입니다. 또한 나의 행동 및 목적에 대해서알릴 수 있고, 상대방이 가지고 있는 포트폴리오를 제한할 의도로 행해질 수 있습니다.  구속력의 3가지 조건 : 최종성(Finality), 구체성(Specificity), 결과성(consequences)  구속력은 유연성을 감소키켜, 상대가 구속력 있는 제안을 못하도록 방지할 수 도 있습니다.  구속력을 만드는 방법          공개적 선언 (Public pronouncement)      외부협력자와 연계하기      요구의 절실함 부각 (increases the prominence of demands)      위협이나 약속을 점차 강화함(reinforce the threat or promise)        구속력을 벗어나는 방법          애초에 없었던 것처럼 행동하라      구속력 있는 제안을 원론적 입장으로 되돌려 표헌하라      상대가 응하지 않을 경우에 관계악화를 최소화하라      상대의 위신을 세워줄 방안을 제시하라      투쟁적 협상의 마무리  대안들을 여러개의 패키지로 제시할 수 있는가? (본능적으로 사람들을 여러 개의 선택사항들을 가지는 것을 좋아합니다.)  차이를 반씩 나누기  제안한 후 신속하게 기한 정하기  동의할 경우 우대조건을 추가로 제시하기  협상에서 Extra로 줘야될 조건은 미리 공개하지 않고, 협상의 마무리 시점을 위해 비축해놔야 합니다.상대가 협상 마무리 시점에 추가로 요구했을때 미리 준비하지 않고, 그 시점에 준비하게 되면 또 너무 많은 양보를 하게될가능성이 높습니다.투쟁적 협상의 강경한 전술들은 어떤 것이 있을까요?  좋은 경찰/나쁜 경찰 (Good Cop/ Bad Cop)          두 경찰관이 번갈아 용의자를 심문할 때, 한명은 악역으로 협박,불쾌한 행동,비타협적인 태도로 심문하고,다른 한명은 나쁜경찰이 오기전에 빨리 합의하자고 착한 역할을 함.단점은 너무 눈에 빤히 보일 수 있음..        하이볼 로우볼 전술          당사자들이 결코 달성할 수 없는 터무니 없는 높은가격이나 낮은 가격을 첫 제안으로 시작함.과도한 제안은 상대로 하여금 자신의 최초가격을 재평가하게 해서 저항가격에 좀더 가까이 다가서도록 할 수 있습니다.단점은 상대가 협상이 시간낭비라고 생각할 수 있고, 협상이 결렬될 수 있습니다.        보기전술          자신에게 별로 중요하지 않거나, 전혀 중요하진 않은 쟁점을 상당히 중요한 것인 척하는 것입니다.이렇게하면 협상 마무리 시점에서 중요한 것처럼 가장했던 쟁점과 실제로 자신에게 중요한 쟁점을 상대와 맞교환하거나 양보할 수 있습니다.        니블전술          협상을 마무리 짓기 위해 이전에 한번도 논의된 바 없는 아이템에 대하여 아주 작은 양보를 요구하는 것입니다.        겁쟁이전술 (치킨게임)          치킨게임은 아마 아실꺼에요 두사람의 운전자가 서로에게 향할때 누가 먼저 핸들을 꺾느냐?에 대한 것 입니다.노사협상에서 많이 사용되고, 파업이나 직장폐쇠등 강경한 조치들을 취했을 때 누가 먼저 입장을 바꾸는지에 대한 것 입니다.단점은 협상이 심각한 게임으로 변질되면서 현실과 협상사이에서 우위를 점하기 위해 꾸민 입장을 구별하기 어려워지는 점 입니다.        위협전술          분노의 모습을 상대에게 보여줘 상대의 감정을 이용하는 방법        호전적 전술          노사분규에서 파업전 붉은 띠를 머리에 두르거나, 머리를 깎고 강경한 분위기를 표출해서 협상의 우위를 점하는 방법        감언이설 전술          너무 많은 정보를 한꺼번에 전달해서 올바른 의사결정을 방해하는 행동. 예를 들면 보험계약에서 복잡한 보험 구조가 있습니다. 보장범위와 금액에 대해서 상당히복잡해서 올바른 결정을 하기에 어려움이 많습니다.      Reference강의 : 비즈니스 협상책 : 최고의 협상 (p150 ~ p233)]]></content>
      <categories>
        
          <category> business </category>
        
      </categories>
      <tags>
        
          <tag> 협상 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[비즈니스 협상 - 협상의 개념과 유형]]></title>
      <url>/business/2016/09/07/2/</url>
      <content type="text"><![CDATA[Overview:대학원에서 비즈니스 협상을 수강하며 정리한 노트 입니다. 수업내용을 정리한 것이라서 일부 부족하거나 잘못된 부분이 있을 수 있습니다. 혹시 잘못된 부분이나 궁금한 것이 있으면 아래 Disqus 댓글을 통해 알려주세요 :)갈등조정 모델 (Dual Concern Model)협상을 하기 위해서는 갈등이 필요합니다. 협상자체가 서로 원하는 것을 얻는것, 즉 갈등이라는 요소가 필요하기 때문이에요그럼 무조건 양보하는사람, 내이익만 주장하는 사람, 회피하는 사람은 협상이 진행될 수 없습니다.!  Yielding - 양보에만 관심 O, 즉 항상양보하기 때문에 공동의 이익이 없게됨 =&gt; 협상발생이 안된다.  Inaction - 무관심, 상대방에게 무관심하기 때문에 공공의 이익이 없음, =&gt; 협상발생이 안된다.  Contending - 자기것에만 관심 있음, Yielding과는 반대이며, 공공이익X, 협상X  Compromising - 조금더 찾아봐야함Negotitaion의 4가지 특징  Interdependency (상호의존성)  Give and Take(Offer-and-Counteroffer) (계속반복된다)  Win-Win (서로 이익이 되어야 함, 1회성이 아닌 지속적 이익발생)  Create Value and Claim Value  (교환하면 가치가 생기는 것, 적절한 교환비율)죄수의 딜레마 (Prisoner’s dilemma)죄수의 딜레마란 ? 두명의 죄수가 자백하고, 자백하지 않는 것에 따라 서로의 이익이 달라지게 되는데, 여기서 최선은 둘다 침묵하는 것이지만,침묵하지 못하고 상대방의 비리를 폭로해 최선의 결과를 얻을 수 없게되는 게임. 즉, 최선의 해가 있다는 것을 서로 알고 있지만(둘다침묵) 서로를 믿지 못하기 때문에 둘다 비리를 폭로하게되는 딜레마를 다룬 게임 입니다. 이런 죄수의 딜레마같은 게임을 “비협조적 모델(이기적 동기가 강함)” 이라고 합니다. 비협조적 모델에서 협조적 모델로 이동시키기 위해서는 어떻게 해야 할까요?크게 3가지 방법이 있습니다.  소통  반복게임 (즉 1회성이 아닌, 계속된 협상/거래가 이루어질 수 있게 함)  보복! (둘중에 한명이 배신했을때에 대한 손해를 크게만들어서 배신하지 못하도록 함)현실에서도 위의 3가지를 이용하고 있는데요, 바로 “단체교섭(노사관계)”에서 사용되고 있습니다.  강제로 단체교섭에 임하게 된다(소통) - 법적으로 정해짐  단체교섭은 2년단위로 반복된다.(반복)  약속을 안지키면 파업할 수 있다. (보복) &lt;-&gt; 회사는 반대로 “직장폐쇠”를 할 수 있습니다.(폐쇠기간 임금지금X)          그래야 Nash solution에 접근할 수 있습니다.  물론 위의 규칙이 있다고 항상 좋은 결과가 나오는 것은 아니지만, 그래도 좋은 결과를 이끌어낼 수 있는 방법이 됩니다.      게임이론과 협상의 차이점?게임이론은 아래의 2가지 가정을 하고 시작하게 됩니다.  가장 합리적인 선택을 한다.  상대방의 전략을 알고 있다. 또한 내가 상대방의 전략을 알고 있다는 것을 상대방도 알고 있다 (Chain rule)하지만 위의 2가지 조건은 현실세계에서는 성립하기 어렵습니다. 우선 협상은 합리적이지 않을때도 있습니다.(인질협상) 그리고 상대방의 전략을 대부분 모르는 경우가 많습니다. 그리고 게임은 나의 이익의 극대화에 초점이 맞춰져 있다면, 협상은 공동이익의 극대화에 맞춰져 있습니다.협상을 하지 말아야 할 때는?  협상보다 더 좋은 선택이 있을때  상대방에 대한 의존도가 높을때 (주로 결과가 좋지 않음)  비윤리적 결과를 만들게 될때  기다리면 상황이 좋아지는 경우  협상의 준비가 미흡한 경우협상의 유형에는 어떤 것이 있나?  일방협상(노사협상)과 쌍방협상일방협상은 쌍방협상으로 변경시켜야 성공가능성이 높게된다. 한쪽만 협상을 원하게되면 해결책을 찾기 어렵다  내부협상과 외부협상  2자간 협상과 다자간의 협상, 다자간협상의 경우 대원칙이 중요하고 결렬될 가능성이 높다.Positional 협상과 Interest 협상 (투쟁적협상과 통합적 협상)Positional 협상을 Interest 협상으로 할수 있도록 유도 해야 함.Positional 협상  구체적 요구조건  하지만, 구체적일 수록 양보가 어렵다.  양보의 이유가 타당해야 한다.  필요하면 압력을 행사하거나 협박을 할 수 있다 –&gt; 장기적 관계파괴Interest 협상  문제해결의 배경이되는 요구를 함(Needs) -&gt; 대안찾기 -&gt; 소통,창의성 -&gt; 해결 -&gt; 장기적 관계가 좋아짐  양측 입장을 이해하고 창의적으로 접근  상호이익을 위해 다수의 옵션을 개발  신뢰와 객관적 기준에 의한 협상이 되도록 함실제 현실에서는 Positional 협상(Bargain)이 더 많이 발생하게 된다. 그래서 Interest 협상이 되도록 변경시켜야 한다.  ex) 임금교섭에서 노동자측이 8%의 임금인상을 요구한다. 그럼 사용자 측은 다른 대안을 제시한다. (유치원 교육비 지원), 교육비 지원등 다양한 옵션으로 임금인상을 억제하고, 대안으로 선택할 수 있게 함. 이렇게 했을때 실제로 나가는 비용은 훨씬 더 적어지게 됨.협상의 조건 - 협상과 Relationship거래적 이슈(매각,분쟁해결,새로운계약체결)와 관계적 이슈(장기적 묙표인식,공동작업,신뢰와 존중의 지속)BATNA ( Best Alternative to a Negotiated Agreement)협상보다 더 좋은 대안이 있을때(Plan B) 협상을 할때는 BATNA를 정하고 임해야 하는데, 협상자체도 중요하지만 언제 협상을 마칠지에 대해서 미리 계획을 세우는 것이 필요하다.즉 대안을 가지고 있으면 협상력이 증가하게 된다. 하지만 협상의 상대방은 어느부분이 BATNA인지 모른다.  ex) 중국의 한 관광지에서 10만원짜리 대형부채를 팔고 있다. 한 사람이 8만원까지 깎았고, 상인은 더 가격을 내리면 판매하지 않겠다고이야기 함(즉 판매하지 않겠다는 한계선이 BATNA 이다.) 하지만 이것은 내가 느끼는 BATNA이고 상대방의 실제 생각은 알 수 없다.관광을 마치고 돌아가는 길에 상인은 8만원 짜리 부채를 2만원에 다른사람에게 판매하였다.하지만 위의 예는 Bargain에 가깝고, 지속적인 관계를 만들어낼 수 없기 때문에 협상이라고 할 수 는 없다.Negotiated Agreement &lt; BATNA 가 되면, 협상이 발생하지 못한다. 왜냐하면 협상을 중단했을때 더 좋은 선택이 있기 때문즉 협상을 하기 위해서는 상호간에 NA &gt; BATNA 가 되도록 양쪽에서 노력을 해야 한다.즉 협상할때는 아래의 계획을 세워야 하는데.  BATNA를 정하고 (언제 협상을 종료할지, 협상의 마지노선)  실제 양보의 최저선(Bottom line of concession)  first offer(최초제안선)을 잘설정해서 협상을 진행해야 한다.Reference강의 : 비즈니스 협상책 : 최고의 협상]]></content>
      <categories>
        
          <category> business </category>
        
      </categories>
      <tags>
        
          <tag> 협상 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[비즈니스 협상 - Negotiation 과 Bargain의 차이]]></title>
      <url>/business/2016/08/31/1/</url>
      <content type="text"><![CDATA[Overview:대학원에서 비즈니스 협상을 수강하며 정리한 노트 입니다. 수업내용을 정리한 것이라서 일부 부족하거나 잘못된 부분이 있을 수 있습니다. 혹시 잘못된 부분이나 궁금한 것이 있으면 아래 Disqus 댓글을 통해 알려주세요 :)Negotiation 과 Bargain우선 사전적의미는 아래와 같습니다.  Negotiation : 협상,교섭,절충,협의  Bargain : 협상하다,(정상가보다)싸게 사는 물건, 합의, 흥정그럼 둘의 가장큰 차이는 뭘까요? 바로 Win-Win 이냐 Win-Lose 이냐의 차이가 있습니다.  Negotiation : Win-Win  Bargain : Win-Lose그럼 Bargain은 무엇인가요?여름 휴가철이면 기쁜마음으로 해수욕장으로 향하죠 그곳에서 겪게되는 바가지가 바로 Bargain의 한 모습이라고 할 수 있습니다.성수기의 바가지 요금은 사실 다신 하고싶지 않은 거래이죠.. 하지만, 그상황에서는 어쩔수없는 거래를 하게 됩니다.거래를 안하게 되는경우도 있구요 여기서 Bargain의 Win-Lose 전략을 확인할 수 있습니다.상인은 바가지요금을 통해 원하는 걸 얻고 (Win), 그 요금을 지불하는 사람은 다신 거래하고 싶지 않을꺼에요(Lose)그럼 협상에서는 Bargain을 사용하지 않나요? 아니에요 사용할 수 도 있습니다. 하지만, 장기적으로 Win-Win해야 협상은 계속될 수 있기때문에Bargain을 사용하진 않는다고 합니다.Negotiation이 발생할 수 있는 예를 들어 볼까요?Negotiation의 핵심은 서로 선호도가 다른 제품/항목/요구를 서로 맞바꾸는데 있어요한국과 미국의 FTA를 보면 한국은 전자제품/자동차를 더 많이 팔고싶어하고, 미국은 농축산물을 더 자유롭게 팔고 싶어하죠이처럼 서로 중요하다고 생각되는 선호항목이 다를때 Negotiation이 발생할 수 있습니다.즉 협상 상대방과 내가 거래할 것이 있을 때 발생합니다. 일방적으로 내가 원하거나, 상대방만 원해서는 협상이 진행되지 않습니다.항상 다른 A,B가 만나면 Negotitation이 발생할 수 있을까요?두 A,B라는 사람이만나서 Negotiation이 일어나기 위해서는 inter-defendancy가 있어야 합니다. 즉 상호간에 Negotitation을 했을때가 안했을때보다 이익이되어야 Negotitaion이 진행될 수 있는 거에요. 즉 거래를 통해 이익일때 서로 거래를 진행하게 됩니다. (당연한건가요?)  A : 협상를 했을때의 이익 &gt; 협상를 하지 않았을때의 이익 B : 협상를 했을때의 이익 &gt; 협상를 하지 않았을때의 이익  A,B는 Inter-defendancy가 있다고 얘기할 수 있어요 A,B는 협상을 했을때의 이익이 하지 않았을 때보다 크기 때문에, 협상을 진행하게 됩니다.  A : 협상를 했을때의 이익 &lt; 협상를 하지 않았을때의 이익 B : 협상를 했을때의 이익 &gt; 협상를 하지 않았을때의 이익  두 A,B는 Inter-defendancy가 없다고 할 수 있습니다. 즉 이럴경우에는, A측에서 협상을 하고 싶어하지 않겠죠? 그래서 협상이 진행되지 않는다고 합니다.그럼 이렇게 협상이 진행되면 항상 Win-Win 일까요?위에서 나타낸 “협상을 했을 때의 이익”은 협상의 결과에 대한 이익은 아니고, 협상의 과정에서의 이익입니다.협상을 진행했을때의 이익이될 가능성에 대해서 이야기하는 것 같아요좋은 협상을 하기위해 필요한 조건들은 어떤 것이 있을까요?전략과 전술과 기술 이 세가지요소가 협상을 잘하기위한 요소 입니다. 협상을 잘 하려면 내가 무조건 다 얻는 것이 아니라, 서로 Win-Win 할 수 있는 구조를 만들어야 합니다.협상과정에서의 갈등(Conflict) 역시 중요한 요소 입니다.양측의 갈등은 협상에서는 긍정적 요소 입니다. 왜냐하면 협상을 진행하기 위해서는 서로 갈등을 해결하기 위해서 협상에 집중하기 때문이에요그래서 이러한 갈등(Conflict)관리 또한 협상에서는 중요한 요소 입니다. 또한 갈등이 너무 커서 폭발할정도라면? 협상은 파행되고 협상자체를 더이상 진행될 수 없게 됩니다. 갈등이 너무 커져서 임계점(폴발할정도)을 넘는다면? 협상은 파행되고 더이상 진행되지 않습니다.그럼 갈등을 어떻게 효과적으로 관리할 수 있을까요? 바로 “offer-counteroffer” 를 지속해야 합니다. 제시하고 제시받고 제시하고 제시받고 를 계속 반복하는 거에요.그럼 “offer-counteroffer”를 어디서 사용할까요?7월에 진행된 최저임금 협상을 예를 들어볼게요 최저임금은 최저임금위원회(공익위원 9명 + 근로자위원 9명 + 사용자위원 9명)으로 구성되어 있습니다. 최저임금 협상은 항상 치열한데요, 2016년의 협상과정을 보면 아래와 같습니다. 최초 제시안 : 사용자위원 - 5580원(동결) 제시 VS 근로자위원 - 1만원 1차 수정안 : 사용자위원 - 5610원 VS 근로자위원 8400원 2차 수정안 : 사용자위원 - 5645원 VS 근로자위원 8200원 3차 수정안 : 사용자위원 - 5715원 VS 근로자위원 8100원물론 이가운데 합의점을 찾지 못하고 중간에 퇴장하는 사건도 있었고, 서로의 의견차이가 너무 극명하였지만,이처럼 서로 계속 요구사항을 전달하고 변경하는 일련의 과정들이 offer-countoffer 라고 할 수 있습니다.Reference책 : 최고의 협상]]></content>
      <categories>
        
          <category> business </category>
        
      </categories>
      <tags>
        
          <tag> 협상 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
